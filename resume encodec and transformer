Short answer: yes — both the EnCodec and the Transformer LM are trained via the same entrypoint: audiocraft.train. The type of training is chosen by the solver:

solver=compression → trains EnCodec (the audio compression model)

solver=musicgen → trains the Transformer LM (MusicGen’s token LM)

You already proved this from your own checkpoints’ xp.cfg:

EnCodec file shows solver: compression

LM file shows solver: musicgen

Your LoRA fine-tune used your custom scripts (lora_train_jsonl.py / lora_infer.py) and does not use audiocraft.train. That’s only for full/base training.

How to launch each (from your repo root)

Important: run as a module and under dora (otherwise you hit “Not in a xp!”).

conda activate py3.10
cd /home/audiocraft

# EnCodec training (compression)
dora run -d . python -m audiocraft.train \
  solver=compression \
  datasource.train=/home/audiocraft/data/train.json \
  datasource.valid=/home/audiocraft/data/valid.json \
  sample_rate=32000 channels=1 \
  dataset.batch_size=32 \
  optim.epochs=200 optim.updates_per_epoch=300

# Transformer LM (MusicGen LM)
dora run -d . python -m audiocraft.train \
  solver=musicgen \
  datasource.train=/home/audiocraft/data/wavs/metadata/train_filtered.jsonl \
  datasource.valid=/home/audiocraft/data/wavs/metadata/valid_filtered.jsonl \
  sample_rate=32000 dataset.batch_size=4 \
  generate.lm.use_sampling=true generate.lm.top_k=250 \
  compression_model_checkpoint=/home/audiocraft/saved_models/repacked_encodec.th

You also have custom configs:

config/solver/compression/encodec_custom_32khz.yaml

config/solver/musicgen/musicgen_custom.yaml

EnCodec (compression) resume

cd /home/audiocraft
PYTHONPATH=. dora run -d . -- \
python -m audiocraft.train \
  solver=compression/encodec_custom_32khz \
  continue_from=/home/audiocraft/saved_models/encodec_25epochs_checkpoint.th


Alternative (without Dora, still fine—just creates local logs):

cd /home/audiocraft
PYTHONPATH=. python -m audiocraft.train \
  solver=compression/encodec_custom_32khz \
  continue_from=/home/audiocraft/saved_models/encodec_25epochs_checkpoint.th

ransformer LM (MusicGen) resume (adapt the continue_from to your LM ckpt):

cd /home/audiocraft
PYTHONPATH=. dora run -d . -- \
python -m audiocraft.train \
  solver=musicgen/musicgen_custom \
  compression_model_checkpoint=/home/audiocraft/saved_models/encodec_90epochs_best.th \
  continue_from=/home/audiocraft/saved_models/transformer_epoch94/checkpoint_xxx.th
