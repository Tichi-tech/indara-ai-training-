# Clone AudioCraft
git clone https://github.com/facebookresearch/audiocraft.git
cd audiocraft

# Install AudioCraft in editable mode
pip install -e .

# Install required libraries
pip install torchaudio flashy dora omegaconf einops julius soundfile tqdm

audiocraft/data/wavs/

import os, json, random
from pathlib import Path

AUDIO_DIR = Path("data/wavs")
MANIFEST_DIR = Path("data/manifest")
MANIFEST_DIR.mkdir(parents=True, exist_ok=True)

all_files = list(AUDIO_DIR.glob("*.wav"))
random.shuffle(all_files)

split = int(0.8 * len(all_files))  # 80% train, 20% valid

def make_manifest(files):
    return [{"path": str(f), "duration": 30.0} for f in files]

with open(MANIFEST_DIR / "train.json", "w") as f:
    json.dump(make_manifest(all_files[:split]), f, indent=2)

with open(MANIFEST_DIR / "valid.json", "w") as f:
    json.dump(make_manifest(all_files[split:]), f, indent=2)

print(f"✅ Created manifest: {len(all_files[:split])} train, {len(all_files[split:])} valid")
✅ Option 1: Use VS Code Remote Upload (Easiest GUI)
If you're already using VS Code (which Jarvis supports):
🔁 Steps:
1.	Click the "VS Code" button from your JarvisLabs instance (from the dashboard)
2.	It will launch Visual Studio Code in the browser, connected to your instance.
3.	In the file sidebar:
o	Right-click a folder (or create a new one inside audiocraft/data/)
o	Choose Upload…
o	Select your merged_audio folder or .wav files from your Mac
o	Upload them directly (drag-and-drop also works in most setups)
✅ That's it — no command line needed.
Initial Setup

  - Environment: AudioCraft repo on A100 GPU with Docker container
  - Goal: Train custom EnCodec model on 1300+ 32kHz meditation audio files

  Key Issues & Solutions

  1. Config Structure

  Problem: Wrong config format and location
  Solution: Created proper solver config at config/solver/compression/encodec_custom_32khz.yaml

  2. Data Format

  Problem: AudioCraft expects JSONL format, not JSON arrays
  Solution:
  # Convert JSON array to JSONL (one object per line)
  python3 -c "import json; data=json.load(open('/home/audiocraft/data/train.json')); [print(json.dumps(item)) for item in 
  data]" > train.jsonl

  3. Missing Required Fields

  Problem: AudioCraft needs sample_rate field in each entry
  Solution:
  sed 's/"duration": 30.0/"duration": 30.0, "sample_rate": 32000/g' train.json > train_fixed.jsonl

  4. Data Split

  Problem: Need separate train/validation sets
  Solution:
  - Train: 1000 files
  - Valid: 200+ files

  5. Multiprocessing Issues

  Problem: Docker container incompatible with PyTorch DataLoader workers
  Solution: dataset.num_workers=0 (single-threaded loading)

  6. Memory Management

  Problem: Initial GPU memory errors
  Solution: dataset.batch_size=16 (conservative for stability)

  Final Working Command

  cd ~/audiocraft
  python -m audiocraft.train solver=compression/encodec_custom_32khz dataset.num_workers=0 dataset.batch_size=16

  Final Config File

  # @package __global__
  defaults:
    - compression/default
    - /model: encodec/encodec_large_nq4_s640
    - override /dset: audio/default
    - _self_
channels: 1
  sample_rate: 32000

  datasource:
    max_sample_rate: 32000
    max_channels: 1
    train: /home/audiocraft/data/train.json
    valid: /home/audiocraft/data/valid.json
    evaluate: /home/audiocraft/data/valid.json
    generate: null

Reflection: From JSONL Upload to Solving Config & Compatibility Issues
1. Uploading and Preparing Dataset Metadata (JSONL)
You prepared your dataset metadata files (train_filtered.jsonl, valid_filtered.jsonl), describing audio samples.

Verified the files exist and are accessible in the expected folder /home/audiocraft/data/wavs/metadata/.

2. Understanding Config Namespace Conflicts (dset vs datasource)
AudioCraft’s config system evolved over versions:

Older versions use datasource.* keys at root.

Newer versions use nested dset.datasource.* keys, with a dset config group.

Mixing these namespaces in CLI overrides caused Hydra validation errors.

3. Creating a Minimal Dataset Config YAML
To avoid Hydra CLI parsing errors and ensure proper config loading, you created a small YAML config config/dset/audio/local.jsonl.yaml.

This YAML explicitly defined all dataset paths and parameters:

yaml
Copy
datasource:
  max_sample_rate: 32000
  max_channels: 1
  train: /home/audiocraft/data/wavs/metadata/train_filtered.jsonl
  valid: /home/audiocraft/data/wavs/metadata/valid_filtered.jsonl
  evaluate: /home/audiocraft/data/wavs/metadata/valid_filtered.jsonl
  generate: /home/audiocraft/data/wavs/metadata/valid_filtered.jsonl
Then, referenced it via dset=audio/local.jsonl in your CLI call.

4. Fixing Hydra/Dora CLI Parsing Limitations
Dora’s Hydra wrapper does not support Hydra’s @file syntax for overrides.

We switched from passing many CLI overrides to:

Either expanding a file with xargs (which caused other errors).

Or better, using a dedicated config YAML file for dataset paths.

5. Solving the “GlobalHydra is already initialized” Error
Error traced to resolve_checkpoint_path() importing audiocraft.train unconditionally.

Importing audiocraft.train triggers a second Hydra initialization, which is forbidden.

We patched resolve_checkpoint_path:

Moved from audiocraft import train inside the conditional branch for //sig/ checkpoints only.

This prevents re-import when loading local checkpoints, eliminating the double-init error.

6. Verifying Config and Dataset Loading
After the fix, the logs showed dataset metadata loading successfully.

Now the training process proceeds to the model loading phase without config-related crashes.

Step 1: Prepare Dataset Metadata YAML File
Create a YAML config describing your dataset paths and parameters, so Hydra loads them properly:
mkdir -p /home/audiocraft/config/dset/audio

cat > /home/audiocraft/config/dset/audio/local.jsonl.yaml << 'EOF'
# @package __global__
datasource:
  max_sample_rate: 32000
  max_channels: 1
  train: /home/audiocraft/data/wavs/metadata/train_filtered.jsonl
  valid: /home/audiocraft/data/wavs/metadata/valid_filtered.jsonl
  evaluate: /home/audiocraft/data/wavs/metadata/valid_filtered.jsonl
  generate: /home/audiocraft/data/wavs/metadata/valid_filtered.jsonl
EOF
Step 2: Fix resolve_checkpoint_path to avoid double Hydra init
Edit /home/audiocraft/audiocraft/utils/checkpoint.py:

Find the function resolve_checkpoint_path and replace its body with this:
def resolve_checkpoint_path(sig_or_path: tp.Union[Path, str], name: tp.Optional[str] = None,
                            use_fsdp: bool = False) -> tp.Optional[Path]:
    """Resolve a given checkpoint path for a provided dora sig or path."""
    sig_or_path = str(sig_or_path)

    if sig_or_path.startswith('//sig/'):
        from audiocraft import train  # only import when needed to avoid Hydra double init
        xps_root = train.main.dora.dir / 'xps'
        sig = sig_or_path[len('//sig/'):]
        path = xps_root / sig
    else:
        path = Path(sig_or_path)
        path = AudioCraftEnvironment.resolve_reference_path(path)

    if path.is_dir():
        path = path / checkpoint_name(name, use_fsdp=use_fsdp)

    return path if path.exists() else None
Step 3: Run training command referencing the YAML config
Use this CLI command without any datasource or dset overrides on CLI except pointing to your config:
python -m audiocraft.train \
  solver=musicgen/musicgen_base_32khz \
  dset=audio/local.jsonl \
  compression_model_checkpoint=/home/audiocraft/saved_models/encodec_90epochs_best.th \
  dataset.batch_size=4 dataset.num_workers=0 \
  optim.epochs=100 generate.every=10 evaluate.every=5
Why this works:
Dataset paths and required params are fully defined in the local.jsonl.yaml config file.

Hydra loads this cleanly under dset=audio/local.jsonl, no CLI namespace clashes.

resolve_checkpoint_path no longer triggers a double Hydra init since train import is conditional.

Training script can now start smoothly loading dataset and checkpoint without config errors or double init errors.

important metric: d-model and codeboos
d_model: 512
n_q: 4 (codebooks)
card: 2048 (vocab size per codebook)
frame_rate: 50
tokens/sec: 200
