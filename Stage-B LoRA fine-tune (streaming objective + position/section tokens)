1) Grab the files I generated

Training segments (ready):
segments_stageB_minimal.jsonl

Train / Val split (whole-track holdout):
• Train → segments_stageB_train.jsonl
• Val → segments_stageB_val.jsonl

Expected codes filenames (one per track):
expected_codes_filenames.txt

Validation report:
segments.validation_report.json

Split summary:

Tracks: 29 total → 25 train / 4 val

Segments: 1,159 train / 185 val (≈ 4.48 h total at 12s)

2) Put your codes where the loader can find them

Choose one location:

Local on Jarvis: /workspace/codes/{track_key}.codes.npz

S3: s3://YOUR-BUCKET/codes/{track_key}.codes.npz

Use the names listed in expected_codes_filenames.txt.
If you only have chunk WAVs, you can tokenize each chunk and concatenate codes in time order per track_key.

3) Project layout on Jarvis
your-repo/
  datasets/
    segments_jsonl.py        # minimal loader (look-back + masking in trainer)
  data/
    segments_train.jsonl     # upload: segments_stageB_train.jsonl
    segments_val.jsonl       # upload: segments_stageB_val.jsonl
  codes/                      # put {track_key}.codes.npz here (or use S3)


Minimal loader (recap of behavior):

Derives track_key from sample_id.

Loads codes_root/{track_key}.codes.npz → codes of shape [n_q, T].

Returns:

ctx = codes[:, start_f - lookback_f : end_f] (includes 4s look-back for non-first)

tgt = codes[:, start_f : end_f] (12s targets only)

4) Environment setup (Jarvis, VS Code terminal)
conda create -n acraft310 python=3.10 -y
conda activate acraft310
pip install torch torchaudio  # match your CUDA
pip install numpy fsspec s3fs smart_open soundfile tqdm

5) Trainer changes you need (concise)

Inputs/labels: shift by 1 in time (next-token objective).

Mask loss on the first lookback_f frames so you only grade the 12s window.

Only use the minimal tokens in this run: [FR] [POS] [POS_BIN].

Pseudocode for loss (drop-in idea):

# ctx: [B, K, T_ctx]  -> K=4 codebooks, T_ctx = lookback+600 (e.g., 800)
x = ctx[..., :-1].transpose(1,2)  # [B, T-1, K]
y = ctx[..., 1: ].transpose(1,2)  # [B, T-1, K]

ignore = -100
mask = torch.ones_like(y[..., 0], dtype=torch.bool)      # [B, T-1]
mask[:, :lookback_f-1] = False                           # ignore lookback region (shift-aware)
y_masked = y.clone()
y_masked[~mask] = ignore

# model(x, plan_tokens) -> logits per codebook head; compute CE with ignore_index=ignore

6) Kick off training (typical LoRA recipe)

LoRA targets: attention q,k,v,o + MLP

LoRA: r=8, α=16, dropout=0.05

Optim: AdamW, lr 1e-4 → 3e-4 (LoRA params only), wd 0.01, warmup 5% + cosine

Batch: effective 8–16 (use grad accumulation)

Epochs: run until val CE/PPL plateaus (with this dataset size, ~10–30 epochs)

7) Quick validation you should track

Val CE/PPL on the val JSONL (185 segments).

Optional “seam” metric: spectral flux at chunk joins (lower = smoother).

8) Generate your 5-minute demo from the trained model

Either:

Single-shot (if your server allows 300s), or

Chunked 25×12s with 4s priming + tiny crossfade (0.8–1.0s).
POS schedule for 25 chunks: START (0–5) → MID (6–18) → END (19–24);
POS_BIN = (20*i)//25 (00..19).
